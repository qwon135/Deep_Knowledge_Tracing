{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d257cd-824a-4921-8b21-c14bada43824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/sarmat/lgbm-stacking-example/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cc2c9a-80ee-4932-b759-7f49c34b9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import custom_train_test_split, make_dataset\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "\n",
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    precsion = precision_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    recall = recall_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    F1_score = f1_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    print('auc :',auc)\n",
    "    print('acc :',acc)\n",
    "    print('precision :',precsion)\n",
    "    print('recall :',recall)\n",
    "\n",
    "def test_to_csv(preds, name:str):\n",
    "    \n",
    "    result = []\n",
    "    for n,i in enumerate(preds):\n",
    "        row = {}    \n",
    "        row['id'] = n\n",
    "        row['prediction'] = i\n",
    "        result.append(row)\n",
    "    pd.DataFrame(result).to_csv(name, index=None)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f4ceb8-7f94-43ca-90e5-1fa66095051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "param1 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 48,\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.12\n",
    "            \n",
    "            }\n",
    "\n",
    "param2 = {   \n",
    "            'boosting': 'gbdt', \n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 48,\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.11\n",
    "            }\n",
    "\n",
    "param3 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 48,\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.10\n",
    "            }\n",
    "\n",
    "param4 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 50,\n",
    "            'metric' : 'auc',\n",
    "            'learning_rate' : 0.09\n",
    "            \n",
    "            }\n",
    "\n",
    "param5 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 52,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 1\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee7f6473-3745-4b22-9fbd-f6dfe18b4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['i_head', 'i_mid','i_tail', 'hour', 'dow']\n",
    "cont_cols = [                        \n",
    "        'user_correct_answer',\n",
    "        'user_total_answer',\n",
    "        'user_acc',            \n",
    "        't_elapsed',            \n",
    "        'cum_correct',\n",
    "        'last_problem',\n",
    "        'head_term',\n",
    "        # 'left_asymptote',\n",
    "        'elo_prob',\n",
    "        'pkt',\n",
    "        'u_head_mean',\n",
    "        'u_head_count',\n",
    "        'u_head_std',\n",
    "        'u_head_elapsed',\n",
    "        'i_mid_elapsed',\n",
    "        'i_mid_mean',\n",
    "        'i_mid_std',\n",
    "        'i_mid_sum',\n",
    "        'i_mid_count',\n",
    "        'i_mid_tag_count',\n",
    "        'assessment_mean',\n",
    "        'assessment_sum',\n",
    "        # 'assessment_std',\n",
    "        'tag_mean',\n",
    "        'tag_sum',\n",
    "        # 'tag_std',\n",
    "        'tail_mean',\n",
    "        'tail_sum',\n",
    "        # 'tail_std',\n",
    "        'hour_mean',\n",
    "        'hour_sum',\n",
    "        # 'hour_std',\n",
    "        'dow_mean',\n",
    "        'dow_sum',\n",
    "        # 'dow_std',\n",
    "        'tag_elapsed',\n",
    "        'tag_elapsed_o',\n",
    "        'tag_elapsed_x',\n",
    "        'assessment_elapsed',\n",
    "        'assessment_elapsed_o',\n",
    "        'assessment_elapsed_x',\n",
    "        'tail_elapsed',\n",
    "        'tail_elapsed_o',\n",
    "        'tail_elapsed_x']\n",
    "\n",
    "FEATS = cat_cols + cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9884ba83-6b5d-425b-be04-c0000a1f3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/all.pkl')\n",
    "\n",
    "valid_user = pd.read_csv('/opt/ml/input/data/cv_valid_data.csv').userID.unique()\n",
    "test_user = pd.read_csv('/opt/ml/input/data/test_data.csv').userID.unique()\n",
    "\n",
    "train = data[data.userID.isin(valid_user)==False]\n",
    "train = data[data.userID.isin(test_user)==False]\n",
    "\n",
    "X_train = data[FEATS]\n",
    "y_train = data.answerCode\n",
    "X_valid = data[data.userID.isin(valid_user)][FEATS]\n",
    "y_valid = data[data.userID.isin(valid_user)].answerCode\n",
    "\n",
    "# train, test = custom_train_test_split(train_data)\n",
    "# y_train, train, y_test, test = make_dataset(train, test)\n",
    "test = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/test_data-1.pkl')\n",
    "test = test[test.answerCode==-1][FEATS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c404f1b8-04c6-49bf-843b-3f3485fd513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1653588, number of negative: 872368\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.349675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6616\n",
      "[LightGBM] [Info] Number of data points in the train set: 2525956, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654638 -> initscore=0.639491\n",
      "[LightGBM] [Info] Start training from score 0.639491\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.42183\tvalid_1's binary_logloss: 0.424774\n",
      "[200]\ttraining's binary_logloss: 0.415734\tvalid_1's binary_logloss: 0.418925\n",
      "[300]\ttraining's binary_logloss: 0.411469\tvalid_1's binary_logloss: 0.414846\n",
      "[400]\ttraining's binary_logloss: 0.407937\tvalid_1's binary_logloss: 0.411542\n",
      "[500]\ttraining's binary_logloss: 0.404909\tvalid_1's binary_logloss: 0.408768\n",
      "[600]\ttraining's binary_logloss: 0.402049\tvalid_1's binary_logloss: 0.406037\n",
      "[700]\ttraining's binary_logloss: 0.399214\tvalid_1's binary_logloss: 0.403451\n",
      "[800]\ttraining's binary_logloss: 0.396685\tvalid_1's binary_logloss: 0.401019\n",
      "[900]\ttraining's binary_logloss: 0.394161\tvalid_1's binary_logloss: 0.398614\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train, label = y_train) # lgb.Dataset(train[FEATS], y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, label = y_valid) # lgb.Dataset(valid[FEATS], y_valid)\n",
    "\n",
    "# num_round? 1000~ 10000\n",
    "model1 = lgb.train(param1, lgb_train, 1500, valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=200,verbose_eval=100, categorical_feature = cat_cols)\n",
    "model2 = lgb.train(param2, lgb_train, 1500, valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=200,verbose_eval=100, categorical_feature = cat_cols)\n",
    "model3 = lgb.train(param3, lgb_train, 1500, valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=200,verbose_eval=100, categorical_feature = cat_cols)\n",
    "model4 = lgb.train(param4, lgb_train, 1500, valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=200,verbose_eval=100, categorical_feature = cat_cols)\n",
    "model5 = lgb.train(param5, lgb_train, 1500, valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=200,verbose_eval=100, categorical_feature = cat_cols)            \n",
    "\n",
    "# 굳이 필요 없을듯? \n",
    "# train_predict1 = model1.predict(train[FEATS])\n",
    "# train_predict2 = model2.predict(train[FEATS])\n",
    "# train_predict3 = model3.predict(train[FEATS])\n",
    "# train_predict4 = model4.predict(train[FEATS])\n",
    "# train_predict5 = model5.predict(train[FEATS])\n",
    "\n",
    "valid_predict1 = model1.predict(X_valid)\n",
    "valid_predict2 = model2.predict(X_valid)\n",
    "valid_predict3 = model3.predict(X_valid)\n",
    "valid_predict4 = model4.predict(X_valid)\n",
    "valid_predict5 = model5.predict(X_valid)\n",
    "\n",
    "test_predict1 = model1.predict(test)\n",
    "test_predict2 = model2.predict(test)\n",
    "test_predict3 = model3.predict(test)\n",
    "test_predict4 = model4.predict(test)\n",
    "test_predict5 = model5.predict(test)\n",
    "\n",
    "# print('Fold no: {}'.format(fold_))\n",
    "print(\"AUC LGB1:{} \".format(metrics.roc_auc_score(y_valid, valid_predict1)))\n",
    "print(\"AUC LGB2:{} \".format(metrics.roc_auc_score(y_valid, valid_predict2)))\n",
    "print(\"AUC LGB3:{} \".format(metrics.roc_auc_score(y_valid, valid_predict3)))\n",
    "print(\"AUC LGB4:{} \".format(metrics.roc_auc_score(y_valid, valid_predict4)))\n",
    "print(\"AUC LGB5:{} \".format(metrics.roc_auc_score(y_valid, valid_predict5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_path = '/opt/ml/level2-dkt-level2-recsys-08/LetsEnsemble/test4feature'\n",
    "valid_path = '/opt/ml/level2-dkt-level2-recsys-08/LetsEnsemble/valid4feature'\n",
    "\n",
    "test_to_csv(valid_predict1, os.path.join(valid_path, 'lgbm_valid_preds01'))\n",
    "test_to_csv(valid_predict2, os.path.join(valid_path, 'lgbm_valid_preds02'))\n",
    "test_to_csv(valid_predict3, os.path.join(valid_path, 'lgbm_valid_preds03'))\n",
    "test_to_csv(valid_predict4, os.path.join(valid_path, 'lgbm_valid_preds04'))\n",
    "test_to_csv(valid_predict5, os.path.join(valid_path, 'lgbm_valid_preds05'))\n",
    "\n",
    "test_to_csv(test_predict1, os.path.join(test_path, 'lgbm_test_preds01'))\n",
    "test_to_csv(test_predict2, os.path.join(test_path, 'lgbm_test_preds02'))\n",
    "test_to_csv(test_predict3, os.path.join(test_path, 'lgbm_test_preds03'))\n",
    "test_to_csv(test_predict4, os.path.join(test_path, 'lgbm_test_preds04'))\n",
    "test_to_csv(test_predict5, os.path.join(test_path, 'lgbm_test_preds05'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6882bc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level2-dkt-level2-recsys-08/LetsEnsemble/blending_lgbm.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444b54227d/opt/ml/level2-dkt-level2-recsys-08/LetsEnsemble/blending_lgbm.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m new_valid_data \u001b[39m=\u001b[39m valid[FEATS]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444b54227d/opt/ml/level2-dkt-level2-recsys-08/LetsEnsemble/blending_lgbm.ipynb#ch0000007vscode-remote?line=1'>2</a>\u001b[0m new_valid_data\u001b[39m.\u001b[39mloc[:,\u001b[39m'\u001b[39m\u001b[39mpredict1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m valid_predict1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22444b54227d/opt/ml/level2-dkt-level2-recsys-08/LetsEnsemble/blending_lgbm.ipynb#ch0000007vscode-remote?line=2'>3</a>\u001b[0m new_valid_data\u001b[39m.\u001b[39mloc[:,\u001b[39m'\u001b[39m\u001b[39mpredict2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m valid_predict2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid' is not defined"
     ]
    }
   ],
   "source": [
    "new_valid_data = X_valid[FEATS].copy()\n",
    "new_valid_data.loc[:,'predict1'] = valid_predict1\n",
    "new_valid_data.loc[:,'predict2'] = valid_predict2\n",
    "new_valid_data.loc[:,'predict3'] = valid_predict3\n",
    "new_valid_data.loc[:,'predict4'] = valid_predict4\n",
    "new_valid_data.loc[:,'predict5'] = valid_predict5\n",
    "\n",
    "\n",
    "valid_tail = new_valid_data[new_valid_data.index.isin(X_valid.groupby('userID').tail(1).index)==True]\n",
    "new_valid_data = new_valid_data[new_valid_data.index.isin(X_valid.groupby('userID').tail(1).index)==False]\n",
    "\n",
    "new_test_data = test[FEATS].copy()\n",
    "new_test_data.loc[:,'predict1'] = test_predict1\n",
    "new_test_data.loc[:,'predict2'] = test_predict2\n",
    "new_test_data.loc[:,'predict3'] = test_predict3\n",
    "new_test_data.loc[:,'predict4'] = test_predict4\n",
    "new_test_data.loc[:,'predict5'] = test_predict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c3b5435a-ab5d-4f37-90d7-0655c15f2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'num_leaves' : 70,\n",
    "        'metric': 'auc',\n",
    "        'num_threads': -1,\n",
    "        'learning_rate' : 0.3,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e8327116-8598-4455-b695-9d18c7e88675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 166077, number of negative: 87196\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8134\n",
      "[LightGBM] [Info] Number of data points in the train set: 253273, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655723 -> initscore=0.644293\n",
      "[LightGBM] [Info] Start training from score 0.644293\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.85415\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.869872\n",
      "auc : 0.8698723693339501\n",
      "acc : 0.7876344086021505\n",
      "precision : 0.770618556701031\n",
      "recall : 0.8125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_tail = y_valid[y_valid.index.isin(X_valid.groupby('userID').tail(1).index)==True]\n",
    "y_new_valid = y_valid[y_valid.index.isin(X_valid.groupby('userID').tail(1).index)==False]\n",
    "\n",
    "lgb_valid = lgb.Dataset(new_valid_data, label = y_new_valid)\n",
    "lgb_tail = lgb.Dataset(valid_tail, label = y_tail)\n",
    "\n",
    "model = lgb.train(\n",
    "                    param, \n",
    "                    lgb_valid,\n",
    "                    valid_sets =[lgb_tail],\n",
    "                    num_boost_round = 1000,\n",
    "                    early_stopping_rounds=100, \n",
    "                    verbose_eval=100,                     \n",
    "                    categorical_feature=cat_cols)                    \n",
    "        \n",
    "Final_valid_predict = model.predict(valid_tail)\n",
    "Final_test_predict = model.predict(new_test_data)\n",
    "\n",
    "# print('Fold no: {}'.format(fold_))\n",
    "get_metric(y_tail, Final_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4e9d7861-8ecc-4520-81bf-6d930f32a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timezone, timedelta\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "time_record = datetime.now(KST)\n",
    "_day = str(time_record)[:10]\n",
    "_time = str(time_record.time())[:8]\n",
    "now_time = _day+'_'+_time\n",
    "\n",
    "test_to_csv(Final_test_predict,f'Ensemble_{now_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef5ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1402265f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4862595879184aec98d66af92eacf41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7ff842547760>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "train_pool = Pool(new_valid_data ,y_new_valid)\n",
    "eval_pool = Pool(valid_tail , y_tail)\n",
    "\n",
    "Final_cat = CatBoostClassifier(\n",
    "            iterations = 1000,\n",
    "            random_seed = 42,\n",
    "            learning_rate = 0.1,\n",
    "            loss_function = 'Logloss', \n",
    "            custom_metric = ['Logloss','AUC'],\n",
    "            early_stopping_rounds = 30,\n",
    "            use_best_model =  True,\n",
    "            task_type = \"GPU\",\n",
    "            bagging_temperature = 1,\n",
    "            verbose = False)\n",
    "\n",
    "Final_cat.fit(train_pool, eval_set=eval_pool, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e789af41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc : 0.8741977913968548\n",
      "acc : 0.7876344086021505\n",
      "precision : 0.7949438202247191\n",
      "recall : 0.7690217391304348\n"
     ]
    }
   ],
   "source": [
    "preds = Final_cat.predict(new_test_data , prediction_type='Probability')[:,1]\n",
    "val_preds = Final_cat.predict(valid_tail , prediction_type='Probability')[:,1]\n",
    "\n",
    "get_metric(y_tail, val_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a15f8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "KST = timezone(timedelta(hours=9))\n",
    "time_record = datetime.now(KST)\n",
    "_day = str(time_record)[:10]\n",
    "_time = str(time_record.time())[:8]\n",
    "now_time = _day+'_'+_time\n",
    "\n",
    "test_to_csv(preds, f'Ensemble_{now_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485efab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
