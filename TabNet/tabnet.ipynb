{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataset import feature_engineering, custom_train_test_split, make_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    precsion = precision_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    recall = recall_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    F1_score = f1_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    print('auc :',auc)\n",
    "    print('acc :',acc)\n",
    "    print('precision :',precsion)\n",
    "    print('recall :',recall)\n",
    "\n",
    "def test_to_csv(preds, name:str):\n",
    "    \n",
    "    result = []\n",
    "    for n,i in enumerate(preds):\n",
    "        row = {}    \n",
    "        row['id'] = n\n",
    "        row['prediction'] = i\n",
    "        result.append(row)\n",
    "    pd.DataFrame(result).to_csv(f'output/{name}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_cols = [\n",
    "            # 'assessmentItemID',\n",
    "            # 'testId',\n",
    "            # 'KnowledgeTag',\n",
    "            'hour',\n",
    "            'dow',\n",
    "            'i_head',\n",
    "            'i_mid',\n",
    "            'i_tail',\n",
    "]\n",
    "cont_cols = [                        \n",
    "            'user_correct_answer',\n",
    "            'user_total_answer',\n",
    "            'user_acc',            \n",
    "            't_elapsed',            \n",
    "            'cum_correct',\n",
    "            # 'last_problem',\n",
    "            'head_term',\n",
    "            # 'left_asymptote',\n",
    "            'elo_prob',\n",
    "            'pkt',\n",
    "            'u_head_mean',\n",
    "            'u_head_count',\n",
    "            'u_head_std',\n",
    "            'u_head_elapsed',\n",
    "            'i_mid_elapsed',\n",
    "            'i_mid_mean',\n",
    "            'i_mid_std',\n",
    "            'i_mid_sum',\n",
    "            'i_mid_count',\n",
    "            'i_mid_tag_count',\n",
    "            'assessment_mean',\n",
    "            'assessment_sum',\n",
    "            # 'assessment_std',\n",
    "            'tag_mean',\n",
    "            'tag_sum',\n",
    "            # 'tag_std',\n",
    "            'tail_mean',\n",
    "            'tail_sum',\n",
    "            # 'tail_std',\n",
    "            'hour_mean',\n",
    "            'hour_sum',\n",
    "            # 'hour_std',\n",
    "            'dow_mean',\n",
    "            'dow_sum',\n",
    "            # 'dow_std',\n",
    "            'tag_elapsed',\n",
    "            'tag_elapsed_o',\n",
    "            'tag_elapsed_x',\n",
    "            'assessment_elapsed',\n",
    "            'assessment_elapsed_o',\n",
    "            'assessment_elapsed_x',\n",
    "            'tail_elapsed',\n",
    "            'tail_elapsed_o',\n",
    "            'tail_elapsed_x']\n",
    "FEATS = cate_cols + cont_cols\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/all.pkl')\n",
    "\n",
    "categorical_dims =  []\n",
    "\n",
    "for cate in cate_cols:\n",
    "    categorical_dims.append(int(train_data[[cate]].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hour', 'dow']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "model01 = TabNetClassifier(\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=0.001),\n",
    "                        scheduler_params={\"step_size\":50,\n",
    "                                            \"gamma\":0.9},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        device_name = DEVICE,                        \n",
    "                        mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "\n",
    "model02 = TabNetClassifier(\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=0.003),\n",
    "                        scheduler_params={\"step_size\":50,\n",
    "                                            \"gamma\":0.9},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        device_name = DEVICE,                        \n",
    "                        mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "\n",
    "model03 = TabNetClassifier(\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=0.005),\n",
    "                        scheduler_params={\"step_size\":50,\n",
    "                                            \"gamma\":0.9},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        device_name = DEVICE,                        \n",
    "                        mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "\n",
    "model04 = TabNetClassifier(\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=0.001),\n",
    "                        scheduler_params={\"step_size\":50,\n",
    "                                            \"gamma\":0.5},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        device_name = DEVICE,                        \n",
    "                        mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                      )\n",
    "\n",
    "model05 = TabNetClassifier(\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=0.005),\n",
    "                        scheduler_params={\"step_size\":50,\n",
    "                                            \"gamma\":0.7},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        device_name = DEVICE,                        \n",
    "                        mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                      )                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_user = pd.read_csv('/opt/ml/input/data/cv_valid_data.csv').userID.unique()\n",
    "\n",
    "valid_idx = train_data[train_data.userID.isin(valid_user)==True].groupby('userID').tail(1).index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data[train_data.index.isin(valid_idx) == False]\n",
    "valid = train_data[train_data.index.isin(valid_idx) == True]\n",
    "\n",
    "X_train = train[FEATS]\n",
    "y_train = train.answerCode.values\n",
    "\n",
    "X_valid = valid[FEATS]\n",
    "y_valid = valid.answerCode.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.56924 | train_auc: 0.82886 | valid_auc: 0.81743 |  0:02:22s\n",
      "epoch 1  | loss: 0.50495 | train_auc: 0.84323 | valid_auc: 0.83397 |  0:04:48s\n",
      "epoch 2  | loss: 0.48799 | train_auc: 0.85232 | valid_auc: 0.83912 |  0:07:14s\n",
      "epoch 3  | loss: 0.47845 | train_auc: 0.85559 | valid_auc: 0.843   |  0:09:36s\n",
      "epoch 4  | loss: 0.47537 | train_auc: 0.85749 | valid_auc: 0.84653 |  0:11:56s\n",
      "epoch 5  | loss: 0.47291 | train_auc: 0.85879 | valid_auc: 0.85227 |  0:14:17s\n",
      "epoch 6  | loss: 0.47092 | train_auc: 0.85947 | valid_auc: 0.85541 |  0:16:38s\n",
      "epoch 7  | loss: 0.4693  | train_auc: 0.86032 | valid_auc: 0.85724 |  0:18:58s\n",
      "epoch 8  | loss: 0.46815 | train_auc: 0.86077 | valid_auc: 0.85529 |  0:21:19s\n",
      "epoch 9  | loss: 0.46743 | train_auc: 0.86135 | valid_auc: 0.85795 |  0:23:41s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_valid_auc = 0.85795\n",
      "Best weights from best epoch are automatically used!\n",
      "epoch 0  | loss: 0.52666 | train_auc: 0.85059 | valid_auc: 0.84065 |  0:02:20s\n",
      "epoch 1  | loss: 0.47806 | train_auc: 0.85709 | valid_auc: 0.84822 |  0:04:39s\n",
      "epoch 2  | loss: 0.47226 | train_auc: 0.8594  | valid_auc: 0.85265 |  0:06:58s\n",
      "epoch 3  | loss: 0.46899 | train_auc: 0.86047 | valid_auc: 0.85099 |  0:09:19s\n",
      "epoch 4  | loss: 0.46651 | train_auc: 0.86165 | valid_auc: 0.85659 |  0:11:43s\n",
      "epoch 5  | loss: 0.46547 | train_auc: 0.86249 | valid_auc: 0.85554 |  0:14:04s\n",
      "epoch 6  | loss: 0.46458 | train_auc: 0.86281 | valid_auc: 0.85898 |  0:16:29s\n",
      "epoch 7  | loss: 0.46366 | train_auc: 0.86338 | valid_auc: 0.85944 |  0:18:51s\n",
      "epoch 8  | loss: 0.46321 | train_auc: 0.86396 | valid_auc: 0.86021 |  0:21:10s\n",
      "epoch 9  | loss: 0.46241 | train_auc: 0.86429 | valid_auc: 0.85858 |  0:23:32s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_valid_auc = 0.86021\n",
      "Best weights from best epoch are automatically used!\n",
      "epoch 0  | loss: 0.50903 | train_auc: 0.85145 | valid_auc: 0.84582 |  0:02:20s\n",
      "epoch 1  | loss: 0.4772  | train_auc: 0.85769 | valid_auc: 0.85057 |  0:04:42s\n",
      "epoch 2  | loss: 0.47124 | train_auc: 0.86007 | valid_auc: 0.84955 |  0:07:03s\n",
      "epoch 3  | loss: 0.46662 | train_auc: 0.86254 | valid_auc: 0.85245 |  0:09:26s\n",
      "epoch 4  | loss: 0.4646  | train_auc: 0.86331 | valid_auc: 0.85217 |  0:11:45s\n",
      "epoch 5  | loss: 0.46279 | train_auc: 0.8642  | valid_auc: 0.85378 |  0:14:06s\n",
      "epoch 6  | loss: 0.46222 | train_auc: 0.8643  | valid_auc: 0.85391 |  0:16:22s\n",
      "epoch 7  | loss: 0.46131 | train_auc: 0.86454 | valid_auc: 0.85807 |  0:18:43s\n",
      "epoch 8  | loss: 0.46073 | train_auc: 0.86494 | valid_auc: 0.85781 |  0:21:04s\n",
      "epoch 9  | loss: 0.46019 | train_auc: 0.86563 | valid_auc: 0.85807 |  0:23:24s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_valid_auc = 0.85807\n",
      "Best weights from best epoch are automatically used!\n",
      "epoch 0  | loss: 0.57429 | train_auc: 0.82462 | valid_auc: 0.83283 |  0:02:21s\n",
      "epoch 1  | loss: 0.50625 | train_auc: 0.84384 | valid_auc: 0.84233 |  0:04:39s\n",
      "epoch 2  | loss: 0.4892  | train_auc: 0.85078 | valid_auc: 0.84499 |  0:07:01s\n",
      "epoch 3  | loss: 0.48158 | train_auc: 0.85436 | valid_auc: 0.85023 |  0:09:21s\n",
      "epoch 4  | loss: 0.47694 | train_auc: 0.85644 | valid_auc: 0.85544 |  0:11:41s\n",
      "epoch 5  | loss: 0.47447 | train_auc: 0.85786 | valid_auc: 0.85943 |  0:14:00s\n",
      "epoch 6  | loss: 0.47171 | train_auc: 0.85926 | valid_auc: 0.85938 |  0:16:20s\n",
      "epoch 7  | loss: 0.47019 | train_auc: 0.86025 | valid_auc: 0.85955 |  0:18:41s\n",
      "epoch 8  | loss: 0.46885 | train_auc: 0.8607  | valid_auc: 0.85846 |  0:21:01s\n",
      "epoch 9  | loss: 0.46809 | train_auc: 0.86144 | valid_auc: 0.85841 |  0:23:20s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_valid_auc = 0.85955\n",
      "Best weights from best epoch are automatically used!\n",
      "epoch 0  | loss: 0.51822 | train_auc: 0.84938 | valid_auc: 0.84556 |  0:02:19s\n",
      "epoch 1  | loss: 0.47693 | train_auc: 0.85732 | valid_auc: 0.8441  |  0:04:38s\n",
      "epoch 2  | loss: 0.47061 | train_auc: 0.86023 | valid_auc: 0.85174 |  0:06:59s\n",
      "epoch 3  | loss: 0.46699 | train_auc: 0.86186 | valid_auc: 0.85398 |  0:09:19s\n",
      "epoch 4  | loss: 0.46472 | train_auc: 0.86337 | valid_auc: 0.85625 |  0:11:40s\n",
      "epoch 5  | loss: 0.46338 | train_auc: 0.8638  | valid_auc: 0.85844 |  0:14:01s\n",
      "epoch 6  | loss: 0.46291 | train_auc: 0.86441 | valid_auc: 0.86126 |  0:16:18s\n",
      "epoch 7  | loss: 0.46255 | train_auc: 0.86425 | valid_auc: 0.86009 |  0:18:35s\n",
      "epoch 8  | loss: 0.46154 | train_auc: 0.86488 | valid_auc: 0.85922 |  0:20:55s\n",
      "epoch 9  | loss: 0.46111 | train_auc: 0.86494 | valid_auc: 0.85921 |  0:23:14s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 6 and best_valid_auc = 0.86126\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model01.fit(  \n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[\n",
    "            (X_train, y_train),\n",
    "            (X_valid, y_valid)],\n",
    "            patience= 5,\n",
    "            batch_size= 2048,\n",
    "            virtual_batch_size = 128,\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs = 10,\n",
    "            weights=1,)\n",
    "\n",
    "model02.fit(  \n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[\n",
    "            (X_train, y_train),\n",
    "            (X_valid, y_valid)],\n",
    "            patience= 5,\n",
    "            batch_size= 2048,\n",
    "            virtual_batch_size = 128,\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs = 10,\n",
    "            weights=1,)\n",
    "\n",
    "\n",
    "model03.fit(  \n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[\n",
    "            (X_train, y_train),\n",
    "            (X_valid, y_valid)],\n",
    "            patience= 5,\n",
    "            batch_size= 2048,\n",
    "            virtual_batch_size = 128,\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs = 10,\n",
    "            weights=1,)\n",
    "\n",
    "model04.fit(  \n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[\n",
    "            (X_train, y_train),\n",
    "            (X_valid, y_valid)],\n",
    "            patience= 5,\n",
    "            batch_size= 2048,\n",
    "            virtual_batch_size = 128,\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs = 10,\n",
    "            weights=1,)            \n",
    "\n",
    "model05.fit(  \n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[\n",
    "            (X_train, y_train),\n",
    "            (X_valid, y_valid)],\n",
    "            patience= 5,\n",
    "            batch_size= 2048,\n",
    "            virtual_batch_size = 128,\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs = 10,\n",
    "            weights=1,)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc : 0.8579512604070305\n",
      "acc : 0.7674731182795699\n",
      "precision : 0.8571428571428571\n",
      "recall : 0.6358695652173914\n",
      "auc : 0.8602133441258095\n",
      "acc : 0.771505376344086\n",
      "precision : 0.8694029850746269\n",
      "recall : 0.6331521739130435\n",
      "auc : 0.8580668940795559\n",
      "acc : 0.7567204301075269\n",
      "precision : 0.850187265917603\n",
      "recall : 0.6168478260869565\n",
      "auc : 0.859548450508788\n",
      "acc : 0.771505376344086\n",
      "precision : 0.8694029850746269\n",
      "recall : 0.6331521739130435\n",
      "auc : 0.8612612742830712\n",
      "acc : 0.7688172043010753\n",
      "precision : 0.8426573426573427\n",
      "recall : 0.654891304347826\n"
     ]
    }
   ],
   "source": [
    "valid_predict01 = model01.predict_proba(X_valid)[:,-1]\n",
    "valid_predict02 = model02.predict_proba(X_valid)[:,-1]\n",
    "valid_predict03 = model03.predict_proba(X_valid)[:,-1]\n",
    "valid_predict04 = model04.predict_proba(X_valid)[:,-1]\n",
    "valid_predict05 = model05.predict_proba(X_valid)[:,-1]\n",
    "\n",
    "get_metric(y_valid,valid_predict01)\n",
    "get_metric(y_valid,valid_predict02)\n",
    "get_metric(y_valid,valid_predict03)\n",
    "get_metric(y_valid,valid_predict04)\n",
    "get_metric(y_valid,valid_predict05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/test_data-1.pkl')\n",
    "test = test[test.answerCode==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict01 = model01.predict_proba(test[FEATS].values)[:,-1]\n",
    "test_predict02 = model02.predict_proba(test[FEATS].values)[:,-1]\n",
    "test_predict03 = model03.predict_proba(test[FEATS].values)[:,-1]\n",
    "test_predict04 = model04.predict_proba(test[FEATS].values)[:,-1]\n",
    "test_predict05 = model05.predict_proba(test[FEATS].values)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_valid = X_valid.copy()\n",
    "new_valid.loc[:,'predict01'] = valid_predict01\n",
    "new_valid.loc[:,'predict02'] = valid_predict02\n",
    "new_valid.loc[:,'predict03'] = valid_predict03\n",
    "new_valid.loc[:,'predict04'] = valid_predict04\n",
    "new_valid.loc[:,'predict05'] = valid_predict05\n",
    "\n",
    "\n",
    "new_test = test.copy()\n",
    "new_test.loc[:,'predict01'] = test_predict01\n",
    "new_test.loc[:,'predict02'] = test_predict02\n",
    "new_test.loc[:,'predict03'] = test_predict03\n",
    "new_test.loc[:,'predict04'] = test_predict04\n",
    "new_test.loc[:,'predict05'] = test_predict05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FEATS = [\n",
    "            'predict01',\n",
    "            'predict02',\n",
    "            'predict03',\n",
    "            'predict04',\n",
    "            'predict05'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final = LogisticRegression(max_iter=2000)\n",
    "Final.fit(new_valid[NEW_FEATS], y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_valid_preds = Final.predict_proba(new_valid[NEW_FEATS])[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc : 0.8621502081406105\n",
      "acc : 0.7728494623655914\n",
      "precision : 0.7900874635568513\n",
      "recall : 0.7364130434782609\n"
     ]
    }
   ],
   "source": [
    "get_metric(y_valid, Final_valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_test_preds = Final.predict_proba(new_test[NEW_FEATS])[:,-1]\n",
    "\n",
    "test_to_csv(Final_test_preds, 'blending_tabnet4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
